#include "object_detection_openvino/objectDetectionOpenvino.hpp"

using namespace InferenceEngine;

/* Initialize the subscribers, the publishers and the inference engine */
ObjectDetectionOpenvino::ObjectDetectionOpenvino(ros::NodeHandle& node, ros::NodeHandle& node_private) : node_(node), nodePrivate_(node_private), 
																				imageTransport_(nodePrivate_), 
																				sync(MySyncPolicy(5), imageSubscriberFilter_, depthSubscriberFilter_, cameraInfoSubscriber_){
	// Initialize ROS parameters
	ROS_INFO("[ObjectDetectionOpenvino] Reading ROS parameters");
	paramsSrv_ = nodePrivate_.advertiseService("params", &ObjectDetectionOpenvino::updateParams, this);
	
	detectionId_ = 0;

	initialize();
	
	std::cout << "[ObjectDetectionOpenvino] InferenceEngine: " << GetInferenceEngineVersion() << std::endl;
	ROS_INFO_ONCE("[ObjectDetectionOpenvino] Loading Inference Engine");
	
	ROS_INFO_ONCE("[ObjectDetectionOpenvino] Device info: ");
	std::cout << core_.GetVersions(deviceTarget_);
	
	// Load extensions for the plugin 
#ifdef WITH_EXTENSIONS
	if (deviceTarget_.find("CPU") != std::string::npos) {
		/**
		 * cpu_extensions library is compiled from "extension" folder containing
		 * custom MKLDNNPlugin layer implementations. These layers are not supported
		 * by mkldnn, but they can be useful for inferring custom topologies.
		**/
		core_.AddExtension(std::make_shared<Extensions::Cpu::CpuExtensions>(), "CPU");
	}
#endif
	
	// Initialize subscribers and publishers
	cameraInfoSubscriber_.subscribe(node_,infoTopic_, 1);
	imageSubscriberFilter_.subscribe(imageTransport_, colorTopic_, 10);
	depthSubscriberFilter_.subscribe(imageTransport_, depthTopic_, 10);
	sync.connectInput(imageSubscriberFilter_, depthSubscriberFilter_, cameraInfoSubscriber_);
	sync.registerCallback(boost::bind(&ObjectDetectionOpenvino::cameraCallback,this,_1,_2,_3));
	
	boundingBoxesPublisher_ = nodePrivate_.advertise<object_detection_openvino::BoundingBoxArray>("bounding_boxes", 1);
	detectionImagePublisher_ = imageTransport_.advertise("detection_image", 1);
	markerPublisher_ = nodePrivate_.advertise<visualization_msgs::MarkerArray>("detection_markers", 1);
	boundingBoxes3dPublisher_ = nodePrivate_.advertise<object_detection_openvino::BoundingBox3dArray>("bounding_boxes3d", 1);

	/* Read IR generated by the Model Optimizer (.xml, .bin, .labels files) */
	// Read network model **/
	ROS_INFO("[ObjectDetectionOpenvino] Loading network files");
	netReader_.ReadNetwork(modelFileName_);
	// Set batch size to 1
	ROS_INFO("[ObjectDetectionOpenvino] Batch size is forced to  1");
	netReader_.getNetwork().setBatchSize(1);
	// Extract model name and load it's weights
	netReader_.ReadWeights(binFileName_);
	// Read labels (if any)
	std::ifstream inputFile(labelFileName_);
	std::copy(std::istream_iterator<std::string>(inputFile), std::istream_iterator<std::string>(), std::back_inserter(this->labels_));

	/* Configuring input and output */
	// Prepare input blobs
	ROS_INFO("[ObjectDetectionOpenvino] Checking that the inputs are as expected");
	inputInfo_ = InputsDataMap(netReader_.getNetwork().getInputsInfo());
	if(networkType_ == "YOLO"){
		InputInfo::Ptr& input = inputInfo_.begin()->second;
		inputName_ = inputInfo_.begin()->first;
		input->setPrecision(Precision::U8);
		input->getInputData()->setLayout(Layout::NCHW);
	}else if(networkType_ == "SSD"){
		for (auto &input : inputInfo_){
			// First input contains images
			if (input.second->getTensorDesc().getDims().size() == 4) {  
				inputName_ = input.first;
				input.second->setPrecision(Precision::U8);
				input.second->getInputData()->setLayout(Layout::NCHW);
			// Second input contains image info
			} else if (input.second->getTensorDesc().getDims().size() == 2) {  
				inputName_ = input.first;
				input.second->setPrecision(Precision::FP32);
			} else {
				throw std::logic_error("Unsupported " + std::to_string(input.second->getTensorDesc().getDims().size()) + "D "
										"input layer '" + input.first + "'. Only 2D and 4D input layers are supported");
			}
		}
	}

	// Prepare output blobs
	ROS_INFO("[ObjectDetectionOpenvino] Checking that the outputs are as expected");
	outputInfo_ = OutputsDataMap(netReader_.getNetwork().getOutputsInfo());
	if(networkType_ == "YOLO"){
		if (outputInfo_.size() != 3 && outputInfo_.size() != 2) {
			throw std::logic_error("Only accepts networks with three (YOLO) or two (tiny-YOLO) outputs");
		}
	}else if(networkType_ == "SSD"){
		if (outputInfo_.size() != 1) {
			throw std::logic_error("Only accepts networks with one output");
		}
	}	
	for (auto &output : outputInfo_) {
		output.second->setPrecision(Precision::FP32);
		output.second->setLayout(Layout::NCHW);
	}

	// Load model to the device 
	ROS_INFO("[ObjectDetectionOpenvino] Loading model to the device");
	ExecutableNetwork network = core_.LoadNetwork(netReader_.getNetwork(), deviceTarget_);
	
	// Create inference requset
	ROS_INFO("[ObjectDetectionOpenvino] Create infer request");
	async_infer_request_curr_ = network.CreateInferRequestPtr();
	async_infer_request_next_ = network.CreateInferRequestPtr();
}

/* Delete all parameteres */
ObjectDetectionOpenvino::~ObjectDetectionOpenvino() {
	nodePrivate_.deleteParam("model_thresh");
	nodePrivate_.deleteParam("model_iou_thresh");
	
	nodePrivate_.deleteParam("model_xml");
	nodePrivate_.deleteParam("model_bin");
	nodePrivate_.deleteParam("model_labels");
	nodePrivate_.deleteParam("model_thresh");
	nodePrivate_.deleteParam("model_type");
	nodePrivate_.deleteParam("device_target");
	
	nodePrivate_.deleteParam("info_topic");
	nodePrivate_.deleteParam("color_topic");
	nodePrivate_.deleteParam("depth_topic");
	nodePrivate_.deleteParam("show_fps");
	
	nodePrivate_.deleteParam("output_image");
	nodePrivate_.deleteParam("output_boxes");
	nodePrivate_.deleteParam("output_markers");
}

/* Update parameters of the node */
bool ObjectDetectionOpenvino::updateParams(std_srvs::Empty::Request &req, std_srvs::Empty::Response &res){
	nodePrivate_.param("model_thresh", thresh_, (float) 0.3);
	nodePrivate_.param("model_iou_thresh", iouThresh_, (float) 0.4);
	
	nodePrivate_.param("model_xml", modelFileName_, std::string("yolov3_tiny_tags.xml"));
	nodePrivate_.param("model_bin", binFileName_, std::string("yolov3_tiny_tags.bin"));
	nodePrivate_.param("model_labels", labelFileName_, std::string("yolov3_tiny_tags.labels"));
	nodePrivate_.param("model_type", networkType_, std::string("YOLO"));
	nodePrivate_.param("device_target", deviceTarget_, std::string("CPU"));
	
	nodePrivate_.param("info_topic", infoTopic_, std::string("/camera/info"));
	nodePrivate_.param("color_topic", colorTopic_, std::string("/camera/color/image_raw"));
	nodePrivate_.param("depth_topic", depthTopic_, std::string("/camera/depth/image_raw"));
	nodePrivate_.param("show_fps", showFPS_, false);
	
	nodePrivate_.param("output_image", outputImage_, true);
	nodePrivate_.param("output_boxes", outputBoxes_, true);
	nodePrivate_.param("output_markers", outputMarkers_, true);

	return true;
}

/* Get color of the class */
int ObjectDetectionOpenvino::getColor(int c, int x, int max){
	float ratio = ((float)x/max)*5;
	int i = floor(ratio);
	int j = ceil(ratio);
	ratio -= i;
	float r = (1-ratio) * colors[i][c] + ratio*colors[j][c];

	return floor(r*255);
}

/* Detection Object constructor */
ObjectDetectionOpenvino::DetectionObject::DetectionObject(double x, double y, double h, double w, int classId, std::string Class, float confidence, float h_scale, float w_scale) {
	this->xmin = static_cast<int>((x - w / 2) * w_scale);
	this->ymin = static_cast<int>((y - h / 2) * h_scale);
	this->xmax = static_cast<int>(this->xmin + w * w_scale);
	this->ymax = static_cast<int>(this->ymin + h * h_scale);
	this->confidence = confidence;
	this->classId = classId;
	this->Class = Class;
}

/* Detection Object constructor without scale */
ObjectDetectionOpenvino::DetectionObject::DetectionObject(double x, double y, double h, double w, int classId, std::string Class, float confidence){
	this->xmin = static_cast<int>(x);
	this->ymin = static_cast<int>(y);
	this->xmax = static_cast<int>(this->xmin + w);
	this->ymax = static_cast<int>(this->ymin + h);
	this->confidence = confidence;
	this->classId = classId;
	this->Class = Class;
}

/* Bounding Box 2d */
object_detection_openvino::BoundingBox ObjectDetectionOpenvino::DetectionObject::BoundingBox(int id){
	object_detection_openvino::BoundingBox boundingBox;
	boundingBox.id = this->id;
	boundingBox.Class = this->Class;
	boundingBox.probability = this->confidence;
	boundingBox.xmin = this->xmin;
	boundingBox.ymin = this->ymin;
	boundingBox.xmax = this->xmax;
	boundingBox.ymax = this->ymax;
	return boundingBox;
}

/* Bounding Box 3d*/
object_detection_openvino::BoundingBox3d ObjectDetectionOpenvino::DetectionObject::BoundingBox3d(int id){
	object_detection_openvino::BoundingBox3d boundingBox;
	boundingBox.id = this->id;
	boundingBox.Class = this->Class;
	boundingBox.probability = this->confidence;
	boundingBox.xmin = this->xMin3d;
	boundingBox.ymin = this->yMin3d;
	boundingBox.zmin = this->zMin3d;
	boundingBox.xmax = this->xMax3d;
	boundingBox.ymax = this->yMax3d;
	boundingBox.zmax = this->zMax3d;
	return boundingBox;
}

/* Operator < for detection object */
bool ObjectDetectionOpenvino::DetectionObject::operator<(const DetectionObject &s2) const {
	return this->confidence < s2.confidence;
}

/* Index for the entry */
int ObjectDetectionOpenvino::EntryIndex(int side, int lcoords, int lclasses, int location, int entry) {
	int n = location / (side * side);
	int loc = location % (side * side);
	return n * side * side * (lcoords + lclasses + 1) + entry * side * side + loc;
}

/* Intersection of bounding boxes */
double ObjectDetectionOpenvino::IntersectionOverUnion(const DetectionObject &box_1, const DetectionObject &box_2) {
	double width_of_overlap_area = fmin(box_1.xmax, box_2.xmax) - fmax(box_1.xmin, box_2.xmin);
	double height_of_overlap_area = fmin(box_1.ymax, box_2.ymax) - fmax(box_1.ymin, box_2.ymin);
	double area_of_overlap;
	if (width_of_overlap_area < 0 || height_of_overlap_area < 0)
		area_of_overlap = 0;
	else
		area_of_overlap = width_of_overlap_area * height_of_overlap_area;
	double box_1_area = (box_1.ymax - box_1.ymin)  * (box_1.xmax - box_1.xmin);
	double box_2_area = (box_2.ymax - box_2.ymin)  * (box_2.xmax - box_2.xmin);
	double area_of_union = box_1_area + box_2_area - area_of_overlap;
	return area_of_overlap / area_of_union;
}

/* Parse Yolo v3 output*/
void ObjectDetectionOpenvino::ParseYOLOV3Output(const CNNLayerPtr &layer, const Blob::Ptr &blob, const unsigned long resizedImgH, const unsigned long resizedImgW, const unsigned long originalImgH, const unsigned long originalImgW, const float threshold,  std::vector<DetectionObject> &objects) {
	// Validating output parameters 
	if (layer->type != "RegionYolo")
		throw std::runtime_error("Invalid output type: " + layer->type + ". RegionYolo expected");
	const int outBlobH = static_cast<int>(blob->getTensorDesc().getDims()[2]);
	const int outBlobW = static_cast<int>(blob->getTensorDesc().getDims()[3]);
	if (outBlobH != outBlobW)
		throw std::runtime_error("Invalid size of output " + layer->name +
		" It should be in NCHW layout and H should be equal to W. Current H = " + std::to_string(outBlobH) +
		", current W = " + std::to_string(outBlobW));

	// Extracting layer parameters 
	auto num = layer->GetParamAsInt("num");
	try { num = layer->GetParamAsInts("mask").size(); } catch (...) {}
	auto coords = layer->GetParamAsInt("coords");
	auto classes = layer->GetParamAsInt("classes");
	std::vector<float> anchors = {10.0, 13.0, 16.0, 30.0, 33.0, 23.0, 30.0, 61.0, 62.0, 45.0, 59.0, 119.0, 116.0, 90.0, 156.0, 198.0, 373.0, 326.0};
	try { anchors = layer->GetParamAsFloats("anchors"); } catch (...) {}
	auto side = outBlobH;
	int anchorOffset = 0;

	if (anchors.size() == 18) {        // YoloV3
		switch (side) {
			case YOLO_SCALE_13:
				anchorOffset = 2 * 6;
				break;
			case YOLO_SCALE_26:
				anchorOffset = 2 * 3;
				break;
			case YOLO_SCALE_52:
				anchorOffset = 2 * 0;
				break;
			default:
				throw std::runtime_error("Invalid output size");
		}
	} else if (anchors.size() == 12) { // tiny-YoloV3
		switch (side) {
			case YOLO_SCALE_13:
				anchorOffset = 2 * 3;
				break;
			case YOLO_SCALE_26:
				anchorOffset = 2 * 0;
				break;
			default:
				throw std::runtime_error("Invalid output size");
		}
	} else {                           // ???
		switch (side) {
			case YOLO_SCALE_13:
				anchorOffset = 2 * 6;
				break;
			case YOLO_SCALE_26:
				anchorOffset = 2 * 3;
				break;
			case YOLO_SCALE_52:
				anchorOffset = 2 * 0;
				break;
			default:
				throw std::runtime_error("Invalid output size");
		}
	}
	auto sideSquare = side * side;
	const float *outputBlob = blob->buffer().as<PrecisionTrait<Precision::FP32>::value_type *>();

	// Parsing YOLO Region output 
	for (int i = 0; i < sideSquare; ++i) {
		int row = i / side;
		int col = i % side;
		for (int n = 0; n < num; ++n) {
			int objIdx = EntryIndex(side, coords, classes, n * side * side + i, coords);
			int boxIdx = EntryIndex(side, coords, classes, n * side * side + i, 0);
			float scale = outputBlob[objIdx];
			if (scale < threshold)
				continue;
			double x = (col + outputBlob[boxIdx + 0 * sideSquare]) / side * resizedImgW;
			double y = (row + outputBlob[boxIdx + 1 * sideSquare]) / side * resizedImgH;
			double height = std::exp(outputBlob[boxIdx + 3 * sideSquare]) * anchors[anchorOffset + 2 * n + 1];
			double width = std::exp(outputBlob[boxIdx + 2 * sideSquare]) * anchors[anchorOffset + 2 * n];
			for (int j = 0; j < classes; ++j) {
				int classIdx = EntryIndex(side, coords, classes, n * sideSquare + i, coords + 1 + j);
				float prob = scale * outputBlob[classIdx];
				if (prob < threshold)
					continue;
				DetectionObject obj(x, y, height, width, j, this->labels_[j], prob,
					static_cast<float>(originalImgH) / static_cast<float>(resizedImgH),
					static_cast<float>(originalImgW) / static_cast<float>(resizedImgW));
				objects.push_back(obj);
			}
		}
	}
}

/* Parse SSD output */
void ObjectDetectionOpenvino::ParseSSDOutput(const InferenceEngine::CNNLayerPtr &layer, const Blob::Ptr &blob, const unsigned long height, const unsigned long width, const float threshold,  std::vector<DetectionObject> &objects){
	// Validating output parameters
	SizeVector outputDims = blob->getTensorDesc().getDims();
	int maxProposalCount = static_cast<int>(blob->getTensorDesc().getDims()[2]);
	const int objectSize = static_cast<int>(blob->getTensorDesc().getDims()[3]);
	
	if (objectSize != 7) {
		throw std::logic_error("Output should have 7 as a last dimension");
	}
	if (outputDims.size() != 4) {
		throw std::logic_error("Incorrect output dimensions for SSD");
	}
	
	// If network assumes default "background" class, having no label
	const int numClasses = layer->GetParamAsInt("num_classes");
	if (static_cast<int>(labels_.size()) != numClasses) {
		if (static_cast<int>(labels_.size()) == (numClasses - 1)){
			labels_.insert(labels_.begin(), "no-label");
		}else{
			labels_.clear();
		}
	}
	
	const float *outputBlob = blob->buffer().as<PrecisionTrait<Precision::FP32>::value_type *>();
	for (int i = 0; i < maxProposalCount; i++) {
		float id = outputBlob[i * objectSize + 0];
		if (id < 0) {
			break;
		}
		
		auto label = static_cast<int>(outputBlob[i * objectSize + 1]);
		float prob = outputBlob[i * objectSize + 2];
		float xmin = outputBlob[i * objectSize + 3] * width;
		float ymin = outputBlob[i * objectSize + 4] * height;
		float xmax = outputBlob[i * objectSize + 5] * width;
		float ymax = outputBlob[i * objectSize + 6] * height;
		
		double width = xmax - xmin;
		double height = ymax - ymin;
		if (prob < threshold)
			continue;
		DetectionObject obj(xmin, ymin, height, width, label, this->labels_[label], prob);
		objects.push_back(obj);
		
	}
}

/* Camera Callback */
void ObjectDetectionOpenvino::cameraCallback(const sensor_msgs::ImageConstPtr& colorImageMsg, const sensor_msgs::ImageConstPtr& depthImageMsg, const sensor_msgs::CameraInfo::ConstPtr& infoMsg){
	ROS_INFO_ONCE("[ObjectDetectionOpenvino] Subscribed to color image topic: %s", colorTopic_.c_str());
	ROS_INFO_ONCE("[ObjectDetectionOpenvino] Subscribed to depth image topic: %s", depthTopic_.c_str());
	ROS_INFO_ONCE("[ObjectDetectionOpenvino] Subscribed to camerainfo topic: %s", infoTopic_.c_str());
	
	object_detection_openvino::BoundingBoxArray boundingBoxes;
	object_detection_openvino::BoundingBox3dArray boundingBoxes3d;
	visualization_msgs::MarkerArray boxMarkerArray;
	
	auto wallclock = std::chrono::high_resolution_clock::now();

	// Read color and depth images
	cv_bridge::CvImagePtr colorFrame, depthFrame;
	try {
		colorFrame = cv_bridge::toCvCopy(colorImageMsg, sensor_msgs::image_encodings::BGR8);
		depthFrame = cv_bridge::toCvCopy(depthImageMsg, sensor_msgs::image_encodings::TYPE_16UC1);
	} catch (cv_bridge::Exception& e) {
		ROS_ERROR("cv_bridge exception: %s", e.what());
		return;
	}
	const size_t width  = (size_t) colorFrame->image.size().width;
	const size_t height = (size_t) colorFrame->image.size().height;
	
	// Read parameters of the camera
	float fx, fy, cx, cy;
	fx = infoMsg->K[0];
	fy = infoMsg->K[4];
	cx = infoMsg->K[2];
	cy = infoMsg->K[5];

	imageHeader_ = colorImageMsg->header;
	colorFrameId_ = colorImageMsg->header.frame_id;
	depthFrameId_ = depthImageMsg->header.frame_id;

	// Copy data from image to input blob
	nextFrame_ = colorFrame->image.clone();
	Blob::Ptr frameBlob = async_infer_request_next_->GetBlob(inputName_);
	matU8ToBlob<uint8_t>(nextFrame_, frameBlob);

	// Load network
	auto t0 = std::chrono::high_resolution_clock::now();
	
	// In the truly Async mode we start the NEXT infer request, while waiting for the CURRENT to complete
	async_infer_request_next_->StartAsync();

	// Delete marker array
	boxMarkerArray.markers.clear();

	if (OK == async_infer_request_curr_->Wait(IInferRequest::WaitMode::RESULT_READY)) {
		// Show FPS
		if (showFPS_ && outputImage_){
			auto t1 = std::chrono::high_resolution_clock::now();
			ms detection = std::chrono::duration_cast<ms>(t1 - t0);

			t0 = std::chrono::high_resolution_clock::now();
			ms wall = std::chrono::duration_cast<ms>(t0 - wallclock);
			wallclock = t0;
		
			std::ostringstream out;
			cv::putText(currFrame_, out.str(), cv::Point2f(0, 25), cv::FONT_HERSHEY_TRIPLEX, 0.6, cv::Scalar(0, 255, 0), 1, cv::LINE_AA);
			out.str("");
			out << "Wallclock time ";
			out << std::fixed << std::setprecision(2) << wall.count() << " ms (" << 1000.f / wall.count() << " fps)";
			cv::putText(currFrame_, out.str(), cv::Point2f(0, 50), cv::FONT_HERSHEY_TRIPLEX, 0.6, cv::Scalar(0, 0, 255), 1, cv::LINE_AA);
			
			out.str("");
			out << "Detection time  : " << std::fixed << std::setprecision(2) << detection.count()
				<< " ms ("
				<< 1000.f / detection.count() << " fps)";
			cv::putText(currFrame_, out.str(), cv::Point2f(0, 75), cv::FONT_HERSHEY_TRIPLEX, 0.6, cv::Scalar(255, 0, 0), 1, cv::LINE_AA);
		}

		// Processing output blobs
		unsigned long resizedImgH = inputInfo_.begin()->second.get()->getDims()[0];
		unsigned long resizedImgW = inputInfo_.begin()->second.get()->getDims()[1];

		// Parsing outputs
		std::vector<DetectionObject> objects;
		for (auto &output : outputInfo_) {
			auto outputName = output.first;
			CNNLayerPtr layer = netReader_.getNetwork().getLayerByName(outputName.c_str());
			Blob::Ptr blob = async_infer_request_curr_->GetBlob(outputName);
			
			if(networkType_ == "YOLO") ParseYOLOV3Output(layer, blob, resizedImgH, resizedImgW, height, width, thresh_, objects);
			else if(networkType_ == "SSD") ParseSSDOutput(layer, blob, height, width, thresh_, objects);
		}

		// Filtering overlapping boxes
		std::sort(objects.begin(), objects.end());
		for (int i = 0; i < objects.size(); ++i) {
			if (objects[i].confidence == 0)
				continue;
			for (int j = i + 1; j < objects.size(); ++j) {
				if (IntersectionOverUnion(objects[i], objects[j]) >= iouThresh_) {
					objects[j].confidence = 0;
				}
			}
		}

		// Format results
		if(outputBoxes_){
			boundingBoxes.header.stamp = ros::Time::now();
			boundingBoxes.header.frame_id = colorFrameId_;
			boundingBoxes.image_header = imageHeader_;
			
			boundingBoxes3d.header.stamp = ros::Time::now();
			boundingBoxes3d.header.frame_id = colorFrameId_;
			boundingBoxes3d.image_header = imageHeader_;
		}
		
		for(auto &object : objects) {
			if (object.confidence < thresh_)
				continue;
			auto label = object.classId;
			float confidence = object.confidence;

			ROS_DEBUG("[ObjectDetectionOpenvino] %s tag (%.2f%%)", this->labels_[label].c_str(), confidence*100);
			
			// Color of the class
			int offset = object.classId * 123457 % COCO_CLASSES;
			float colorRGB[3];
			
			colorRGB[0] = getColor(2,offset,COCO_CLASSES);
			colorRGB[1] = getColor(1,offset,COCO_CLASSES);
			colorRGB[2] = getColor(0,offset,COCO_CLASSES);
			
			// Extract 3d coordinates from depth image
			// We extract the middle point of the bounding box
			// TO-DO: Improve the object septh using the histogram
			float Xc = (object.xmax + object.xmin) / 2;
			float Yc = (object.ymax + object.ymin) / 2;
			float Zc = 0.001 * (float)depthFrame->image.at<u_int16_t>(cv::Point(Yc,Xc));

			geometry_msgs::Pose poseMin, poseMax;
			poseMin.position.x = (object.xmin - cx) * Zc / fx;
			poseMin.position.y = (object.ymin - cy) * Zc / fy;
			poseMin.position.z = Zc;
			poseMax.position.x = (object.ymax - cx) * Zc / fx;
			poseMax.position.y = (object.ymax - cy) * Zc / fy;
			poseMax.position.z = Zc;
			
			// Image
			if(outputImage_){
				std::ostringstream conf;
				conf << ":" << std::fixed << std::setprecision(3) << confidence;
				std::string labelText = (label < this->labels_.size() ? this->labels_[label] : std::string("label #") + std::to_string(label)) + conf.str();
				
				cv::rectangle(currFrame_, cv::Point2f(object.xmin-1, object.ymin), cv::Point2f(object.xmin + 180, object.ymin - 22), cv::Scalar(colorRGB[2], colorRGB[1], colorRGB[0]), cv::FILLED, cv::LINE_AA);
				cv::putText(currFrame_, labelText, cv::Point2f(object.xmin, object.ymin - 5), cv::FONT_HERSHEY_COMPLEX_SMALL, 1, cv::Scalar(0, 0, 0), 1.5, cv::LINE_AA);
				cv::rectangle(currFrame_, cv::Point2f(object.xmin, object.ymin), cv::Point2f(object.xmax, object.ymax), cv::Scalar(colorRGB[2], colorRGB[1], colorRGB[0]), 4, cv::LINE_AA);
			}
			
			// Markers 
			if(outputMarkers_){
				visualization_msgs::Marker marker = createBoundingBox3dMarker(detectionId_, poseMin, poseMax, colorRGB, depthFrameId_, ros::Time::now());
				boxMarkerArray.markers.push_back(marker);
				marker = createLabel3dMarker(detectionId_*10, this->labels_[label].c_str(), poseMin, poseMax, colorRGB, depthFrameId_, ros::Time::now());
				boxMarkerArray.markers.push_back(marker);
			}
			
			// Bounding boxes 2d and 3d
			if(outputBoxes_){
				// Bounding box 2d
				object_detection_openvino::BoundingBox boundingBox = object.BoundingBox(detectionId_);
				boundingBoxes.bounding_boxes.push_back(boundingBox);
				
				// Bounding box 3d
				object.xMin3d = poseMin.position.x;		object.xMax3d = poseMax.position.x;
				object.yMin3d = poseMin.position.y;		object.yMax3d = poseMax.position.y;
				object.zMin3d = poseMin.position.z;		object.zMax3d = poseMax.position.z;

				object_detection_openvino::BoundingBox3d boundingBox3d = object.BoundingBox3d(detectionId_);
				boundingBoxes3d.bounding_boxes.push_back(boundingBox3d);
			}
			
			detectionId_++;
		}
	}
	
	// Publish
	if(outputImage_) publishImage(currFrame_);
	if(outputBoxes_){
		boundingBoxesPublisher_.publish(boundingBoxes);
		boundingBoxes3dPublisher_.publish(boundingBoxes3d);
	}
	if(outputMarkers_) markerPublisher_.publish(boxMarkerArray);
	
	// In the truly Async mode we swap the NEXT and CURRENT requests for the next iteration
	currFrame_ = nextFrame_;
	nextFrame_ = cv::Mat();
	async_infer_request_curr_.swap(async_infer_request_next_);
	
	return;
}

/* Create 3d Bounding Box for the object */
visualization_msgs::Marker ObjectDetectionOpenvino::createBoundingBox3dMarker(int id, geometry_msgs::Pose poseMin, geometry_msgs::Pose poseMax, float colorRGB[3], std::string targetFrame, ros::Time stamp){
	visualization_msgs::Marker marker;
	marker.header.frame_id = targetFrame;
	marker.header.stamp = stamp;
	marker.ns = "boundingBox3d";
	marker.id = id;
	marker.type = visualization_msgs::Marker::CUBE;
	marker.action = visualization_msgs::Marker::ADD;
	marker.lifetime = ros::Duration(0.15);
	marker.pose.position.x = (poseMax.position.x + poseMin.position.x) / 2.0;
	marker.pose.position.y = (poseMax.position.y + poseMin.position.y) / 2.0;
	marker.pose.position.z = poseMin.position.z;
	marker.pose.orientation.x = 0.0;
	marker.pose.orientation.y = 0.0;
	marker.pose.orientation.z = 0.0;
	marker.pose.orientation.w = 1.0;
	marker.scale.x = poseMax.position.x - poseMin.position.x;
	marker.scale.y = poseMax.position.y - poseMin.position.y;
	marker.scale.z = 0.3;
	marker.color.r = colorRGB[0] / 255.0;
	marker.color.g = colorRGB[1] / 255.0;
	marker.color.b = colorRGB[2] / 255.0;
	marker.color.a = 0.2f;
	
	return marker;
}

/* Create 3d label for the object */
visualization_msgs::Marker ObjectDetectionOpenvino::createLabel3dMarker(int id, std::string label, geometry_msgs::Pose poseMin, geometry_msgs::Pose poseMax, float colorRGB[3], std::string targetFrame, ros::Time stamp){
	visualization_msgs::Marker marker;
	marker.header.frame_id = targetFrame;
	marker.header.stamp = stamp;
	marker.ns = "label3d";
	marker.id = id;
	marker.text = label;
	marker.type = visualization_msgs::Marker::TEXT_VIEW_FACING;
	marker.action = visualization_msgs::Marker::ADD;
	marker.lifetime = ros::Duration(0.15);
	marker.pose.position.x = poseMin.position.x+0.3;
	marker.pose.position.y = poseMin.position.y;
	marker.pose.position.z = poseMin.position.z + 0.05;
	marker.pose.orientation.x = 0.0;
	marker.pose.orientation.y = 0.0;
	marker.pose.orientation.z = 0.0;
	marker.pose.orientation.w = 1.0;
	marker.scale.z = 0.3;
	marker.color.r = colorRGB[0] / 255.0;
	marker.color.g = colorRGB[1] / 255.0;
	marker.color.b = colorRGB[2] / 255.0;
	marker.color.a = 0.8f;
	
	return marker;
}

/* Publish image */
void ObjectDetectionOpenvino::publishImage(cv::Mat image){
	sensor_msgs::Image outputImageMsg;
	
	outputImageMsg.header.stamp = ros::Time::now();
	outputImageMsg.header.frame_id = colorFrameId_;
	outputImageMsg.height = image.rows;
	outputImageMsg.width = image.cols;
	outputImageMsg.encoding = "bgr8";
	outputImageMsg.is_bigendian = false;
	outputImageMsg.step = image.cols * 3;
	size_t size = outputImageMsg.step * image.rows;
	outputImageMsg.data.resize(size);
	memcpy((char*)(&outputImageMsg.data[0]), image.data, size);
	
	detectionImagePublisher_.publish(outputImageMsg);
}

/* Main */
int main(int argc, char** argv){
	ros::init(argc, argv, "object_detection_openvino");
	ros::NodeHandle node("");
	ros::NodeHandle node_private("~");
	
	try{
		ROS_INFO("[ObjectDetectionOpenvino] Initializing node");
		ObjectDetectionOpenvino yoloDetector(node, node_private);
		ros::spin();
	}catch(const char* s){
		ROS_FATAL_STREAM("[ObjectDetectionOpenvino] " << s);
	}catch(...){
		ROS_FATAL_STREAM("[ObjectDetectionOpenvino] Unexpected error");
	}
}
