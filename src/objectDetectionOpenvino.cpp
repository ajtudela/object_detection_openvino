/*
 * OBJECT DETECTION OPENVINO ROS NODE
 *
 * Copyright (c) 2020 Alberto José Tudela Roldán <ajtudela@gmail.com>
 * 
 * This file is part of object_detection_openvino project.
 * 
 * All rights reserved.
 *
 */

#include "object_detection_openvino/objectDetectionOpenvino.hpp"

using namespace InferenceEngine;

/* Initialize the subscribers, the publishers and the inference engine */
ObjectDetectionOpenvino::ObjectDetectionOpenvino(ros::NodeHandle& node, ros::NodeHandle& node_private) : node_(node), nodePrivate_(node_private), 
																				imageTransport_(nodePrivate_), 
																				sync(MySyncPolicy(5), imageSubscriberFilter_, depthSubscriberFilter_, cameraInfoSubscriber_){
	// Initialize ROS parameters
	ROS_INFO("[ObjectDetectionOpenvino] Reading ROS parameters");
	paramsSrv_ = nodePrivate_.advertiseService("params", &ObjectDetectionOpenvino::updateParams, this);
	
	detectionId_ = 0;

	initialize();
	
	std::cout << "[ObjectDetectionOpenvino] InferenceEngine: " << GetInferenceEngineVersion() << std::endl;
	ROS_INFO_ONCE("[ObjectDetectionOpenvino] Loading Inference Engine");
	
	ROS_INFO_ONCE("[ObjectDetectionOpenvino] Device info: ");
	std::cout << core_.GetVersions(deviceTarget_);
	
	// Load extensions for the plugin 
#ifdef WITH_EXTENSIONS
	if (deviceTarget_.find("CPU") != std::string::npos) {
		/**
		 * cpu_extensions library is compiled from "extension" folder containing
		 * custom MKLDNNPlugin layer implementations. These layers are not supported
		 * by mkldnn, but they can be useful for inferring custom topologies.
		**/
		core_.AddExtension(std::make_shared<Extensions::Cpu::CpuExtensions>(), "CPU");
	}
#endif
	
	// Initialize subscribers and publishers
	cameraInfoSubscriber_.subscribe(node_,infoTopic_, 1);
	imageSubscriberFilter_.subscribe(imageTransport_, colorTopic_, 10);
	depthSubscriberFilter_.subscribe(imageTransport_, depthTopic_, 10);
	sync.connectInput(imageSubscriberFilter_, depthSubscriberFilter_, cameraInfoSubscriber_);
	sync.registerCallback(boost::bind(&ObjectDetectionOpenvino::cameraCallback,this,_1,_2,_3));
	
	boundingBoxesPublisher_ = nodePrivate_.advertise<object_detection_openvino::BoundingBoxArray>(boundingBoxTopic_, 1);
	detectionImagePublisher_ = imageTransport_.advertise(imageDetectedTopic_, 1);
	boundingBoxes3dPublisher_ = nodePrivate_.advertise<object_detection_openvino::BoundingBox3dArray>(boundingBox3dTopic_, 1);
	markerPublisher_ = nodePrivate_.advertise<visualization_msgs::MarkerArray>("detection_markers", 1);

	/* Read IR generated by the Model Optimizer (.xml, .bin, .labels files) */
	// Read network model
	ROS_INFO("[ObjectDetectionOpenvino] Loading network files");
	if(!boost::filesystem::exists(modelFileName_)){
		ROS_FATAL("Network file doesn't exist.");
		ros::shutdown();
	}
	cnnNetwork_ = core_.ReadNetwork(modelFileName_, binFileName_);
	// Read labels (if any)
	std::ifstream inputFile(labelFileName_);
	std::copy(std::istream_iterator<std::string>(inputFile), std::istream_iterator<std::string>(), std::back_inserter(this->labels_));

	/* Configuring input and output */
	// Prepare input blobs
	ROS_INFO("[ObjectDetectionOpenvino] Checking that the inputs are as expected");
	inputInfo_ = InputsDataMap(cnnNetwork_.getInputsInfo());
	if(networkType_ == "YOLO"){
		if (inputInfo_.size() != 1) {
			ROS_FATAL("Only accepts networks that have only one input");
			ros::shutdown();
		}
		InputInfo::Ptr& input = inputInfo_.begin()->second;
		inputName_ = inputInfo_.begin()->first;
		input->setPrecision(Precision::U8);
		input->getInputData()->setLayout(Layout::NCHW);
	}else if(networkType_ == "SSD"){
		if (inputInfo_.size() != 1 && inputInfo_.size() != 2 ){
			ROS_FATAL("Only accepts networks with 1 or 2 inputs");
			ros::shutdown();
		}
		for (auto &input : inputInfo_){
			// First input contains images
			if (input.second->getTensorDesc().getDims().size() == 4) {  
				inputName_ = input.first;
				input.second->setPrecision(Precision::U8);
				input.second->getInputData()->setLayout(Layout::NCHW);
			// Second input contains image info
			} else if (input.second->getTensorDesc().getDims().size() == 2) {  
				inputName_ = input.first;
				input.second->setPrecision(Precision::FP32);
			} else {
				throw std::logic_error("Unsupported " + std::to_string(input.second->getTensorDesc().getDims().size()) + "D "
										"input layer '" + input.first + "'. Only 2D and 4D input layers are supported");
				ros::shutdown();
			}
		}
	}

	// Set batch size to 1
	ROS_INFO("[ObjectDetectionOpenvino] Batch size is forced to  1");
	ICNNNetwork::InputShapes inputShapes = cnnNetwork_.getInputShapes();
	SizeVector& inSizeVector = inputShapes.begin()->second;
	inSizeVector[0] = 1; 
	cnnNetwork_.reshape(inputShapes);
	
	// Prepare output blobs
	ROS_INFO("[ObjectDetectionOpenvino] Checking that the outputs are as expected");
	outputInfo_ = OutputsDataMap(cnnNetwork_.getOutputsInfo());
	if(networkType_ == "YOLO"){
		if (outputInfo_.size() != 3 && outputInfo_.size() != 2) {
			ROS_FATAL("Only accepts networks with three (YOLO) or two (tiny-YOLO) outputs");
			ros::shutdown();
		}
		
		for (auto &output : outputInfo_) {
			output.second->setPrecision(Precision::FP32);
			output.second->setLayout(Layout::NCHW);
		}
		
		if (auto ngraphFunction = cnnNetwork_.getFunction()) {
			for (const auto op : ngraphFunction->get_ops()) {
				auto outputLayer = outputInfo_.find(op->get_friendly_name());
				if (outputLayer != outputInfo_.end()) {
					auto regionYolo = std::dynamic_pointer_cast<ngraph::op::RegionYolo>(op);
					if (!regionYolo) {
						throw std::runtime_error("Invalid output type: " +
							std::string(regionYolo->get_type_info().name) + ". RegionYolo expected");
					}
					yoloParams_[outputLayer->first] = YoloParams(regionYolo);
				}
			}
		}
		else {
			ROS_FATAL("Can't get ngraph::Function. Make sure the provided model is in IR version 10 or greater.");
			ros::shutdown();
		}
	}else if(networkType_ == "SSD"){
		if (outputInfo_.size() != 1) {
			throw std::logic_error("Only accepts networks with one output");
		}

		for (auto &output : outputInfo_) {
			output.second->setPrecision(Precision::FP32);
			output.second->setLayout(Layout::NCHW);
		}
	}

	// Load model to the device 
	ROS_INFO("[ObjectDetectionOpenvino] Loading model to the device");
	ExecutableNetwork network = core_.LoadNetwork(cnnNetwork_, deviceTarget_);
	
	// Create inference request
	ROS_INFO("[ObjectDetectionOpenvino] Create infer request");
	async_infer_request_curr_ = network.CreateInferRequestPtr();
	async_infer_request_next_ = network.CreateInferRequestPtr();
}

/* Delete all parameteres */
ObjectDetectionOpenvino::~ObjectDetectionOpenvino() {
	nodePrivate_.deleteParam("model_thresh");
	nodePrivate_.deleteParam("model_iou_thresh");
	
	nodePrivate_.deleteParam("model_xml");
	nodePrivate_.deleteParam("model_bin");
	nodePrivate_.deleteParam("model_labels");
	nodePrivate_.deleteParam("model_thresh");
	nodePrivate_.deleteParam("model_type");
	nodePrivate_.deleteParam("device_target");
	
	nodePrivate_.deleteParam("info_topic");
	nodePrivate_.deleteParam("color_topic");
	nodePrivate_.deleteParam("depth_topic");
	nodePrivate_.deleteParam("image_detected_topic");
	nodePrivate_.deleteParam("bounding_box_topic");
	nodePrivate_.deleteParam("bounding_box3d_topic");
	nodePrivate_.deleteParam("show_fps");
	
	nodePrivate_.deleteParam("output_image");
	nodePrivate_.deleteParam("output_boxes");
	nodePrivate_.deleteParam("output_markers");
}

/* Update parameters of the node */
bool ObjectDetectionOpenvino::updateParams(std_srvs::Empty::Request &req, std_srvs::Empty::Response &res){
	nodePrivate_.param("model_thresh", thresh_, (float) 0.3);
	nodePrivate_.param("model_iou_thresh", iouThresh_, (float) 0.4);
	
	nodePrivate_.param("model_xml", modelFileName_, std::string("yolov3_tiny_tags.xml"));
	nodePrivate_.param("model_bin", binFileName_, std::string("yolov3_tiny_tags.bin"));
	nodePrivate_.param("model_labels", labelFileName_, std::string("yolov3_tiny_tags.labels"));
	nodePrivate_.param("model_type", networkType_, std::string("YOLO"));
	nodePrivate_.param("device_target", deviceTarget_, std::string("CPU"));
	
	nodePrivate_.param("info_topic", infoTopic_, std::string("/camera/info"));
	nodePrivate_.param("color_topic", colorTopic_, std::string("/camera/color/image_raw"));
	nodePrivate_.param("depth_topic", depthTopic_, std::string("/camera/depth/image_raw"));
	nodePrivate_.param("image_detected_topic", imageDetectedTopic_, std::string("detected_image"));
	nodePrivate_.param("bounding_box_topic", boundingBoxTopic_, std::string("bounding_boxes"));
	nodePrivate_.param("bounding_box3d_topic", boundingBox3dTopic_, std::string("bounding_boxes3d"));
	nodePrivate_.param("show_fps", showFPS_, false);
	
	nodePrivate_.param("output_image", outputImage_, true);
	nodePrivate_.param("output_boxes", outputBoxes_, true);
	nodePrivate_.param("output_markers", outputMarkers_, true);

	return true;
}

/* Get color of the class */
int ObjectDetectionOpenvino::getColor(int c, int x, int max){
	float ratio = ((float)x/max)*5;
	int i = floor(ratio);
	int j = ceil(ratio);
	ratio -= i;
	float r = (1-ratio) * colors[i][c] + ratio*colors[j][c];

	return floor(r*255);
}

/* Detection Object constructor */
ObjectDetectionOpenvino::DetectionObject::DetectionObject(double x, double y, double h, double w, int classId, std::string Class, float confidence, float h_scale, float w_scale) {
	this->xmin = static_cast<int>((x - w / 2) * w_scale);
	this->ymin = static_cast<int>((y - h / 2) * h_scale);
	this->xmax = static_cast<int>(this->xmin + w * w_scale);
	this->ymax = static_cast<int>(this->ymin + h * h_scale);
	this->confidence = confidence;
	this->classId = classId;
	this->Class = Class;
}

/* Detection Object constructor without scale */
ObjectDetectionOpenvino::DetectionObject::DetectionObject(double x, double y, double h, double w, int classId, std::string Class, float confidence){
	this->xmin = static_cast<int>(x);
	this->ymin = static_cast<int>(y);
	this->xmax = static_cast<int>(this->xmin + w);
	this->ymax = static_cast<int>(this->ymin + h);
	this->confidence = confidence;
	this->classId = classId;
	this->Class = Class;
}

/* Bounding Box 2d */
object_detection_openvino::BoundingBox ObjectDetectionOpenvino::DetectionObject::BoundingBox(int id){
	object_detection_openvino::BoundingBox boundingBox;
	boundingBox.id = this->id;
	boundingBox.Class = this->Class;
	boundingBox.confidence = this->confidence;
	boundingBox.roi.x_offset = this->xmin;
	boundingBox.roi.y_offset = this->ymin;
	boundingBox.roi.height = this->ymax - this->xmin;
	boundingBox.roi.width = this->xmax - this->ymin;
	
	if(this->xmin == 0 && this->ymin == 0) boundingBox.roi.do_rectify = false;
	else boundingBox.roi.do_rectify = true;
	
	return boundingBox;
}

/* Bounding Box 3d*/
object_detection_openvino::BoundingBox3d ObjectDetectionOpenvino::DetectionObject::BoundingBox3d(int id){
	object_detection_openvino::BoundingBox3d boundingBox;
	boundingBox.id = this->id;
	boundingBox.Class = this->Class;
	boundingBox.confidence = this->confidence;
	boundingBox.xmin = this->xMin3d;
	boundingBox.ymin = this->yMin3d;
	boundingBox.zmin = this->zMin3d;
	boundingBox.xmax = this->xMax3d;
	boundingBox.ymax = this->yMax3d;
	boundingBox.zmax = this->zMax3d;
	return boundingBox;
}

/* Operator < for detection object */
bool ObjectDetectionOpenvino::DetectionObject::operator<(const DetectionObject &s2) const {
	return this->confidence < s2.confidence;
}

/* Operator > for detection object */
bool ObjectDetectionOpenvino::DetectionObject::operator>(const DetectionObject &s2) const {
	return this->confidence > s2.confidence;
}

/* Index for the entry */
int ObjectDetectionOpenvino::EntryIndex(int side, int lcoords, int lclasses, int location, int entry) {
	int n = location / (side * side);
	int loc = location % (side * side);
	return n * side * side * (lcoords + lclasses + 1) + entry * side * side + loc;
}

/* Intersection of bounding boxes */
double ObjectDetectionOpenvino::IntersectionOverUnion(const DetectionObject &box_1, const DetectionObject &box_2) {
	double width_of_overlap_area = fmin(box_1.xmax, box_2.xmax) - fmax(box_1.xmin, box_2.xmin);
	double height_of_overlap_area = fmin(box_1.ymax, box_2.ymax) - fmax(box_1.ymin, box_2.ymin);
	double area_of_overlap;
	if (width_of_overlap_area < 0 || height_of_overlap_area < 0)
		area_of_overlap = 0;
	else
		area_of_overlap = width_of_overlap_area * height_of_overlap_area;
	double box_1_area = (box_1.ymax - box_1.ymin)  * (box_1.xmax - box_1.xmin);
	double box_2_area = (box_2.ymax - box_2.ymin)  * (box_2.xmax - box_2.xmin);
	double area_of_union = box_1_area + box_2_area - area_of_overlap;
	return area_of_overlap / area_of_union;
}

/* Parse Yolo v3 output*/
void ObjectDetectionOpenvino::ParseYOLOV3Output(const YoloParams &params, const std::string &outputName, const Blob::Ptr &blob, const unsigned long resizedImgH, const unsigned long resizedImgW, const unsigned long originalImgH, const unsigned long originalImgW, const float threshold,  std::vector<DetectionObject> &objects) {
	// Validating output parameters 
	const int outBlobH = static_cast<int>(blob->getTensorDesc().getDims()[2]);
	const int outBlobW = static_cast<int>(blob->getTensorDesc().getDims()[3]);
	if (outBlobH != outBlobW)
		throw std::runtime_error("Invalid size of output " + outputName +
		" It should be in NCHW layout and H should be equal to W. Current H = " + std::to_string(outBlobH) +
		", current W = " + std::to_string(outBlobW));
	
	auto side = outBlobH;
	auto sideSquare = side * side;
	const float *outputBlob = blob->buffer().as<PrecisionTrait<Precision::FP32>::value_type *>();
	
	// Parsing YOLO Region output 
	for (int i = 0; i < sideSquare; ++i){
		int row = i / side;
		int col = i % side;
		for (int n = 0; n < params.num; ++n){
			int objIdx = EntryIndex(side, params.coords, params.classes, n * side * side + i, params.coords);
			int boxIdx = EntryIndex(side, params.coords, params.classes, n * side * side + i, 0);
			float scale = outputBlob[objIdx];
			if (scale < threshold)
				continue;
			double x = (col + outputBlob[boxIdx + 0 * sideSquare]) / side * resizedImgW;
			double y = (row + outputBlob[boxIdx + 1 * sideSquare]) / side * resizedImgH;
			double height = std::exp(outputBlob[boxIdx + 3 * sideSquare]) * params.anchors[2 * n + 1];
			double width = std::exp(outputBlob[boxIdx + 2 * sideSquare]) * params.anchors[2 * n];
			for (int j = 0; j < params.classes; ++j) {
				int classIdx = EntryIndex(side, params.coords, params.classes, n * sideSquare + i, params.coords + 1 + j);
				float prob = scale * outputBlob[classIdx];
				if (prob < threshold)
					continue;
				DetectionObject obj(x, y, height, width, j, this->labels_[j], prob,
					static_cast<float>(originalImgH) / static_cast<float>(resizedImgH),
					static_cast<float>(originalImgW) / static_cast<float>(resizedImgW));
				objects.push_back(obj);
			}
		}
	}
}

/* Parse SSD output */
void ObjectDetectionOpenvino::ParseSSDOutput(const InferenceEngine::CNNLayerPtr &layer, const Blob::Ptr &blob, const unsigned long height, const unsigned long width, const float threshold,  std::vector<DetectionObject> &objects){
	// Validating output parameters
	SizeVector outputDims = blob->getTensorDesc().getDims();
	int maxProposalCount = static_cast<int>(blob->getTensorDesc().getDims()[2]);
	const int objectSize = static_cast<int>(blob->getTensorDesc().getDims()[3]);
	
	if (objectSize != 7) {
		throw std::logic_error("Output should have 7 as a last dimension");
	}
	if (outputDims.size() != 4) {
		throw std::logic_error("Incorrect output dimensions for SSD");
	}
	
	// If network assumes default "background" class, having no label
	/*const int numClasses = layer->GetParamAsInt("num_classes");
	if (static_cast<int>(labels_.size()) != numClasses) {
		if (static_cast<int>(labels_.size()) == (numClasses - 1)){
			labels_.insert(labels_.begin(), "no-label");
		}else{
			labels_.clear();
		}
	}*/
	
	const float *outputBlob = blob->buffer().as<PrecisionTrait<Precision::FP32>::value_type *>();
	for (int i = 0; i < maxProposalCount; i++) {
		float id = outputBlob[i * objectSize + 0];
		if (id < 0) {
			break;
		}
		
		auto label = static_cast<int>(outputBlob[i * objectSize + 1]);
		float prob = outputBlob[i * objectSize + 2];
		float xmin = outputBlob[i * objectSize + 3] * width;
		float ymin = outputBlob[i * objectSize + 4] * height;
		float xmax = outputBlob[i * objectSize + 5] * width;
		float ymax = outputBlob[i * objectSize + 6] * height;
		
		double newWidth = xmax - xmin;
		double newHeight = ymax - ymin;
		if (prob < threshold)
			continue;
		DetectionObject obj(xmin, ymin, newHeight, newWidth, label, this->labels_[label], prob);
		objects.push_back(obj);
	}
}

/* Camera Callback */
void ObjectDetectionOpenvino::cameraCallback(const sensor_msgs::ImageConstPtr& colorImageMsg, const sensor_msgs::ImageConstPtr& depthImageMsg, const sensor_msgs::CameraInfo::ConstPtr& infoMsg){
	ROS_INFO_ONCE("[ObjectDetectionOpenvino] Subscribed to color image topic: %s", colorTopic_.c_str());
	ROS_INFO_ONCE("[ObjectDetectionOpenvino] Subscribed to depth image topic: %s", depthTopic_.c_str());
	ROS_INFO_ONCE("[ObjectDetectionOpenvino] Subscribed to camerainfo topic: %s", infoTopic_.c_str());
	
	object_detection_openvino::BoundingBoxArray boundingBoxes;
	object_detection_openvino::BoundingBox3dArray boundingBoxes3d;
	visualization_msgs::MarkerArray boxMarkerArray;
	
	auto wallclock = std::chrono::high_resolution_clock::now();

	// Read color and depth images
	cv_bridge::CvImagePtr colorFrame, depthFrame;
	try {
		colorFrame = cv_bridge::toCvCopy(colorImageMsg, sensor_msgs::image_encodings::BGR8);
		depthFrame = cv_bridge::toCvCopy(depthImageMsg, sensor_msgs::image_encodings::TYPE_16UC1);
	} catch (cv_bridge::Exception& e) {
		ROS_ERROR("cv_bridge exception: %s", e.what());
		return;
	}
	const size_t width  = (size_t) colorFrame->image.size().width;
	const size_t height = (size_t) colorFrame->image.size().height;
	
	/// 3d
	depthFrame_ = depthFrame->image.clone();
	
	// Read parameters of the camera
	float fx, fy, cx, cy;
	fx = infoMsg->K[0];
	fy = infoMsg->K[4];
	cx = infoMsg->K[2];
	cy = infoMsg->K[5];

	imageHeader_ = colorImageMsg->header;
	colorFrameId_ = colorImageMsg->header.frame_id;
	depthFrameId_ = depthImageMsg->header.frame_id;

	// Copy data from image to input blob
	nextFrame_ = colorFrame->image.clone();
	Blob::Ptr frameBlob = async_infer_request_next_->GetBlob(inputName_);
	matU8ToBlob<uint8_t>(nextFrame_, frameBlob);

	// Load network
	auto t0 = std::chrono::high_resolution_clock::now();
	
	// In the truly Async mode we start the NEXT infer request, while waiting for the CURRENT to complete
	async_infer_request_next_->StartAsync();

	if (OK == async_infer_request_curr_->Wait(IInferRequest::WaitMode::RESULT_READY)) {
		// Show FPS
		if (showFPS_ && outputImage_){
			auto t1 = std::chrono::high_resolution_clock::now();
			ms detection = std::chrono::duration_cast<ms>(t1 - t0);

			t0 = std::chrono::high_resolution_clock::now();
			ms wall = std::chrono::duration_cast<ms>(t0 - wallclock);
			wallclock = t0;
		
			std::ostringstream out;
			cv::putText(currFrame_, out.str(), cv::Point2f(0, 25), cv::FONT_HERSHEY_TRIPLEX, 0.6, cv::Scalar(0, 255, 0), 1, cv::LINE_AA);
			out.str("");
			out << "Wallclock time ";
			out << std::fixed << std::setprecision(2) << wall.count() << " ms (" << 1000.f / wall.count() << " fps)";
			cv::putText(currFrame_, out.str(), cv::Point2f(0, 50), cv::FONT_HERSHEY_TRIPLEX, 0.6, cv::Scalar(0, 0, 255), 1, cv::LINE_AA);
			
			out.str("");
			out << "Detection time  : " << std::fixed << std::setprecision(2) << detection.count()
				<< " ms ("
				<< 1000.f / detection.count() << " fps)";
			cv::putText(currFrame_, out.str(), cv::Point2f(0, 75), cv::FONT_HERSHEY_TRIPLEX, 0.6, cv::Scalar(255, 0, 0), 1, cv::LINE_AA);
		}

		// Processing output blobs of the CURRENT request
		const TensorDesc& inputDesc = inputInfo_.begin()->second.get()->getTensorDesc();
		unsigned long resizedImgH = getTensorHeight(inputDesc);
		unsigned long resizedImgW = getTensorWidth(inputDesc);

		// Parsing outputs
		std::vector<DetectionObject> objects;
		for (auto &output : outputInfo_) {
			auto outputName = output.first;
			CNNLayerPtr layer = cnnNetwork_.getLayerByName(outputName.c_str());
			Blob::Ptr blob = async_infer_request_curr_->GetBlob(outputName);
			
			if(networkType_ == "YOLO") ParseYOLOV3Output(yoloParams_[outputName], outputName, blob, resizedImgH, resizedImgW, height, width, thresh_, objects);
			else if(networkType_ == "SSD") ParseSSDOutput(layer, blob, height, width, thresh_, objects);
		}

		// Filtering overlapping boxes
		std::sort(objects.begin(), objects.end(), std::greater<DetectionObject>());
		for (int i = 0; i < objects.size(); ++i) {
			if (objects[i].confidence == 0)
				continue;
			for (int j = i + 1; j < objects.size(); ++j) {
				if (IntersectionOverUnion(objects[i], objects[j]) >= iouThresh_) {
					objects[j].confidence = 0;
				}
			}
		}

		// Format results
		if(outputBoxes_){
			boundingBoxes.header.frame_id = colorFrameId_;
			boundingBoxes.header.stamp = ros::Time::now();
			boundingBoxes.image_header = imageHeader_;
			
			boundingBoxes3d.header.frame_id = colorFrameId_;
			boundingBoxes3d.header.stamp = ros::Time::now();
			boundingBoxes3d.image_header = imageHeader_;
		}
		
		for(auto &object : objects) {
			if (object.confidence < thresh_)
				continue;
			auto label = object.classId;
			float confidence = object.confidence;

			ROS_DEBUG("[ObjectDetectionOpenvino] %s tag (%.2f%%)", this->labels_[label].c_str(), confidence*100);
			
			// Color of the class
			int offset = object.classId * 123457 % COCO_CLASSES;
			float colorRGB[3];
			colorRGB[0] = getColor(2,offset,COCO_CLASSES);
			colorRGB[1] = getColor(1,offset,COCO_CLASSES);
			colorRGB[2] = getColor(0,offset,COCO_CLASSES);
			
			// Improve bounding box
			object.xmin = object.xmin < 0 ? 0 : object.xmin;
			object.ymin = object.ymin < 0 ? 0 : object.ymin;
			object.xmax = object.xmax > width ? width : object.xmax;
			object.ymax = object.ymax > height ? height : object.ymax;
			
			/// Extract 3d coordinates from depth image
			// Calculate average and standard deviation from depth image
			cv::Mat subdepthP = depthFrame_(cv::Rect(object.xmin, object.ymin, object.xmax-object.xmin, object.ymax-object.ymin));
			cv::Mat subdepth;
			subdepthP.convertTo(subdepth, CV_8U, 0.0390625);
			cv::Scalar m = cv::mean(subdepth);
			cv::threshold(subdepth, subdepth, m[0] * 2.0, 100, 4);

			std::vector<std::vector<cv::Point> > contours;
			std::vector<cv::Vec4i> hierarchy;

			cv::findContours(subdepth, contours, hierarchy, cv::RETR_EXTERNAL, cv::CHAIN_APPROX_SIMPLE, cv::Point(0,0));
			for (int i = 0; i < contours.size(); i++){
				cv::drawContours(subdepth, contours, i, 0xffff, cv::FILLED, 8, hierarchy, 0, cv::Point());
			}
			subdepth.convertTo(subdepth, CV_16U);
			subdepth = subdepth + subdepth * 256;
			cv::bitwise_and(subdepth, subdepthP, subdepthP);
			cv::Mat mask = cv::Mat(subdepthP != 0);
			cv::Scalar avg, dstd;
			cv::meanStdDev(subdepthP, avg, dstd, mask);
			
			// We extract the middle point of the bounding box
			// TO-DO: Improve the object depth using the histogram
			float Xc = (object.xmax + object.xmin) / 2;
			float Yc = (object.ymax + object.ymin) / 2;
			float Zc = 0.001 * (float)depthFrame->image.at<u_int16_t>(cv::Point(Yc,Xc));
			//float Zc = avg[0]/1000.0;
			
			ROS_INFO("(x, y, z: %f, %f, %f)", Xc, Yc, Zc);

			geometry_msgs::Pose poseMin, poseMax;
			poseMin.position.x = (Xc - cx) * Zc / fx;
			poseMin.position.y = (Yc - cy) * Zc / fy;
			poseMin.position.z = Zc;
			poseMax.position.x = (object.xmax - object.xmin) * Zc / fx;
			poseMax.position.y = (object.ymax - object.ymin) * Zc / fy;
			//poseMax.position.z = dstd[0]/1000.0;
			
			
			/// Histogram
			/*int histSize = 256;
			float range[] = { 0, histSize }; //the upper boundary is exclusive
			const float* histRange = { range };
			bool uniform = true, accumulate = false;
			cv::Mat hist, hist_real;
			cv::Mat mask1 = cv::Mat::zeros(depthFrame_.size(), CV_8UC1);
			cv::calcHist(&depthFrame_, 1, 0, cv::Mat(), hist, 1, &histSize, &histRange, uniform, accumulate);
			hist_real = hist.clone();

			int hist_w = 512, hist_h = 400;
			int bin_w = cvRound((double)hist_w / histSize);
			cv::Mat histImage(hist_h, hist_w, CV_8UC3, cv::Scalar(0, 0, 0));
			normalize(hist, hist, 0, histImage.rows, cv::NORM_MINMAX, -1, cv::Mat());
			
			for (int i = 1; i < histSize; i++){
				cv::line(histImage, cv::Point(bin_w * (i - 1), hist_h - cvRound(hist.at<float>(i - 1))),
					cv::Point(bin_w * (i), hist_h - cvRound(hist.at<float>(i))),
					cv::Scalar(255, 0, 0), 2, 8, 0);
				
			}
			cv::imshow("imagen_entrada", depthFrame_);
			cv::imshow("calcHist Demo", histImage);
			cv::waitKey(1);*/

			// Image
			if(outputImage_){
				std::ostringstream conf;
				conf << ":" << std::fixed << std::setprecision(3) << confidence;
				std::string labelText = (label < this->labels_.size() ? this->labels_[label] : std::string("label #") + std::to_string(label)) + conf.str();
				
				cv::rectangle(currFrame_, cv::Point2f(object.xmin-1, object.ymin), cv::Point2f(object.xmin + 180, object.ymin - 22), cv::Scalar(colorRGB[2], colorRGB[1], colorRGB[0]), cv::FILLED, cv::LINE_AA);
				cv::putText(currFrame_, labelText, cv::Point2f(object.xmin, object.ymin - 5), cv::FONT_HERSHEY_COMPLEX_SMALL, 1, cv::Scalar(0, 0, 0), 1.5, cv::LINE_AA);
				cv::rectangle(currFrame_, cv::Point2f(object.xmin, object.ymin), cv::Point2f(object.xmax, object.ymax), cv::Scalar(colorRGB[2], colorRGB[1], colorRGB[0]), 4, cv::LINE_AA);
			}
			
			// Markers 
			if(outputMarkers_){
				visualization_msgs::Marker marker = createBoundingBox3dMarker(detectionId_, poseMin, poseMax, colorRGB, depthFrameId_, ros::Time::now());
				boxMarkerArray.markers.push_back(marker);
				marker = createLabel3dMarker(detectionId_*10, this->labels_[label].c_str(), poseMin, poseMax, colorRGB, depthFrameId_, ros::Time::now());
				boxMarkerArray.markers.push_back(marker);
			}
			
			// Bounding boxes 2d and 3d
			if(outputBoxes_){
				// Bounding box 2d
				object_detection_openvino::BoundingBox boundingBox = object.BoundingBox(detectionId_);
				boundingBoxes.bounding_boxes.push_back(boundingBox);
				
				// Bounding box 3d
				object.xMin3d = poseMin.position.x;		object.xMax3d = poseMax.position.x;
				object.yMin3d = poseMin.position.y;		object.yMax3d = poseMax.position.y;
				object.zMin3d = poseMin.position.z;		object.zMax3d = poseMax.position.z;

				object_detection_openvino::BoundingBox3d boundingBox3d = object.BoundingBox3d(detectionId_);
				boundingBoxes3d.bounding_boxes.push_back(boundingBox3d);
			}
			
			detectionId_++;
		}
	}
	
	// Publish
	if(outputImage_) publishImage(currFrame_);
	if(outputBoxes_){
		boundingBoxesPublisher_.publish(boundingBoxes);
		boundingBoxes3dPublisher_.publish(boundingBoxes3d);
	}
	if(outputMarkers_) markerPublisher_.publish(boxMarkerArray);
	
	// In the truly Async mode we swap the NEXT and CURRENT requests for the next iteration
	currFrame_ = nextFrame_;
	nextFrame_ = cv::Mat();
	async_infer_request_curr_.swap(async_infer_request_next_);
	
	// Delete marker array
	boxMarkerArray.markers.clear();
	
	return;
}

/* Create 3d Bounding Box for the object */
visualization_msgs::Marker ObjectDetectionOpenvino::createBoundingBox3dMarker(int id, geometry_msgs::Pose poseMin, geometry_msgs::Pose poseMax, float colorRGB[3], std::string targetFrame, ros::Time stamp){
	visualization_msgs::Marker marker;
	marker.header.frame_id = targetFrame;
	marker.header.stamp = stamp;
	marker.ns = "boundingBox3d";
	marker.id = id;
	marker.type = visualization_msgs::Marker::CUBE;
	marker.action = visualization_msgs::Marker::ADD;
	marker.lifetime = ros::Duration(0.15);
	marker.pose.position.x = poseMin.position.x ;
	marker.pose.position.y = poseMin.position.y;
	marker.pose.position.z = poseMin.position.z;
	/*marker.pose.position.x = (poseMax.position.x + poseMin.position.x) / 2.0;
	marker.pose.position.y = (poseMax.position.y + poseMin.position.y) / 2.0;
	marker.pose.position.z = poseMin.position.z;*/
	marker.pose.orientation.x = 0.0;
	marker.pose.orientation.y = 0.0;
	marker.pose.orientation.z = 0.0;
	marker.pose.orientation.w = 1.0;
	marker.scale.x = 0.3;
	marker.scale.y = 0.3;
	marker.scale.z = 0.3;
	marker.color.r = colorRGB[0] / 255.0;
	marker.color.g = colorRGB[1] / 255.0;
	marker.color.b = colorRGB[2] / 255.0;
	marker.color.a = 0.2f;
	
	return marker;
}

/* Create 3d label for the object */
visualization_msgs::Marker ObjectDetectionOpenvino::createLabel3dMarker(int id, std::string label, geometry_msgs::Pose poseMin, geometry_msgs::Pose poseMax, float colorRGB[3], std::string targetFrame, ros::Time stamp){
	visualization_msgs::Marker marker;
	marker.header.frame_id = targetFrame;
	marker.header.stamp = stamp;
	marker.ns = "label3d";
	marker.id = id;
	marker.text = label;
	marker.type = visualization_msgs::Marker::TEXT_VIEW_FACING;
	marker.action = visualization_msgs::Marker::ADD;
	marker.lifetime = ros::Duration(0.15);
	marker.pose.position.x = poseMin.position.x + 0.3;
	marker.pose.position.y = poseMin.position.y;
	marker.pose.position.z = poseMin.position.z + 0.05;
	marker.pose.orientation.x = 0.0;
	marker.pose.orientation.y = 0.0;
	marker.pose.orientation.z = 0.0;
	marker.pose.orientation.w = 1.0;
	marker.scale.z = 0.3;
	marker.color.r = colorRGB[0] / 255.0;
	marker.color.g = colorRGB[1] / 255.0;
	marker.color.b = colorRGB[2] / 255.0;
	marker.color.a = 0.8f;
	
	return marker;
}

/* Publish image */
void ObjectDetectionOpenvino::publishImage(cv::Mat image){
	sensor_msgs::Image outputImageMsg;
	
	outputImageMsg.header.stamp = ros::Time::now();
	outputImageMsg.header.frame_id = colorFrameId_;
	outputImageMsg.height = image.rows;
	outputImageMsg.width = image.cols;
	outputImageMsg.encoding = "bgr8";
	outputImageMsg.is_bigendian = false;
	outputImageMsg.step = image.cols * 3;
	size_t size = outputImageMsg.step * image.rows;
	outputImageMsg.data.resize(size);
	memcpy((char*)(&outputImageMsg.data[0]), image.data, size);
	
	detectionImagePublisher_.publish(outputImageMsg);
}

/* Main */
int main(int argc, char** argv){
	ros::init(argc, argv, "object_detection_openvino");
	ros::NodeHandle node("");
	ros::NodeHandle node_private("~");
	
	try{
		ROS_INFO("[ObjectDetectionOpenvino] Initializing node");
		ObjectDetectionOpenvino yoloDetector(node, node_private);
		ros::spin();
	}catch(const char* s){
		ROS_FATAL_STREAM("[ObjectDetectionOpenvino] " << s);
	}catch(...){
		ROS_FATAL_STREAM("[ObjectDetectionOpenvino] Unexpected error");
	}
}
