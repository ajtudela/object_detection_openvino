/*
 * OPENVINO CLASS
 *
 * Copyright (c) 2020-2022 Alberto José Tudela Roldán <ajtudela@gmail.com>
 * 
 * This file is part of object_detection_openvino project.
 * 
 * All rights reserved.
 *
 */

#ifndef OBJECT_DETECTION_OPENVINO__OPENVINO_HPP_
#define OBJECT_DETECTION_OPENVINO__OPENVINO_HPP_

// C++
#include <map>
#include <string>
#include <vector>

// OpenVINO
#include <inference_engine.hpp>
#include <samples/ocv_common.hpp>

#ifdef WITH_EXTENSIONS
    #include <ext_list.hpp>
#endif

// Object detection
#include "object_detection_openvino/detectionObject.hpp"
#include "object_detection_openvino/yoloParams.hpp"

/**
 * @brief Implementation of the Intel OpenVino as a more standard API.
 * 
 */
class Openvino{
	public:

		/**
		 * @brief Constructor.
		 * 
		 */
		Openvino(std::string node_name);

		/**
		 * @brief Destructor.
		 * 
		 */
		~Openvino();

		/**
		 * @brief Set the target device to load the neural network model.
		 * 
		 * @param device The name of the device.
		 */
		void set_target_device(std::string device);

		/**
		 * @brief Read the Intermediate Representation (IR) generated by the Model Optimizer to be loaded into the device.
		 * 
		 * @param model_filename The filename of the model in XML format.
		 * @param bin_filename The filename of the model in BIN format.
		 * @param label_filename The filename of the labels used in the model.
		 */
		void set_network_model(std::string model_filename, std::string bin_filename, std::string label_filename);

		/**
		 * @brief Configure the inputs and outputs of the neural network.
		 * 
		 * @param network_type The name of the neural network.
		 */
		void configure_network(std::string network_type);

		/**
		 * @brief Load the neural network model to the selected device.
		 * 
		 * @param device The name of the device.
		 */
		void load_model_to_device(std::string device);

		/**
		 * @brief Create an asynchronous inference request.
		 * 
		 */
		void create_async_infer_request();

		/**
		 * @brief Start the asynchronous inference request in the next frame.
		 * 
		 */
		void start_next_async_infer_request();

		/**
		 * @brief Swap the asynchronous inference request between the current frame and the next one.
		 * 
		 */
		void swap_async_infer_request();

		/**
		 * @brief Check if the device is ready.
		 * 
		 * @return True if the device is ready. False, otherwise.
		 */
		bool is_device_ready();

		/**
		 * @brief Get the labels of the neural network model.
		 * 
		 * @return The names of the labels.
		 */
		std::vector<std::string> get_labels();

		/**
		 * @brief Get the detected objects in the frame.
		 * 
		 * @param height Height in pixels of the bounding box surrounding the object.
		 * @param width Width in pixels of the bounding box surrounding the object.
		 * @param threshold Value below which objects will be discarded.
		 * @param iou_threshold Value of the intersection over union threshold in a range between 0 and 1.
		 * Above this value, the bounding boxes will be overlapped.
		 * @return The detected objects.
		 */
		std::vector<DetectionObject> get_detection_objects(size_t height, 
														size_t width, 
														float threshold, 
														float iou_threshold);

		/**
		 * @brief Convert the frame to an asynchronous inference request.
		 * 
		 * @param frame The frame to be converted.
		 * @param auto_resize Value to change the size of the frame.
		 */
		void frame_to_next_infer(const cv::Mat &frame, bool auto_resize = false);

	private:

		/// Name of the node.
		std::string node_name_;

		/// Name of the input layer.
		std::string input_name_;

		/// Type of the neural network.
		std::string network_type_;

		/// Name of the device to load the neural network into.
		std::string device_target_;

		/// Implementation of the coordinates of regions for YOLOv3 model.
		std::map<std::string, YoloParams> yolo_params_;

		/// Interface of an executable network.
		InferenceEngine::ExecutableNetwork inf_network_;

		/// Interface of asynchronous current infer request.
		InferenceEngine::InferRequest::Ptr async_infer_request_current_;

		/// Interface of asynchronous next infer request.
		InferenceEngine::InferRequest::Ptr async_infer_request_next_;

		/// A collection that contains string as key, and OutputInfo smart pointer as value.
		InferenceEngine::OutputsDataMap output_info_;

		/// A collection that contains string as key, and InputInfo smart pointer as value.
		InferenceEngine::InputsDataMap input_info_;

		/// Information about the Neural Network and the related binary information. 
		InferenceEngine::CNNNetwork cnn_network_;

		/// Inference Engine Core entity.
		InferenceEngine::Core core_;

		/// Class labels of the neural network.
		std::vector<std::string> labels_;

		/**
		 * @brief Index of the entry.
		 * 
		 * @param side Height of the image. 
		 * @param lcoords Coordinates of the object.
		 * @param lclasses Number of classes in the neural network.
		 * @param location The location.
		 * @param entry The entry.
		 * @return Index of the entry.
		 */
		static int entry_index(int side, int lcoords, int lclasses, int location, int entry);

		/**
		 * @brief Intersection over union of the bounding boxes.
		 * 
		 * @param box_1 First bounding box.
		 * @param box_2 Second boudning box.
		 * @return Ratio between the area of the overlap and area of the union. 
		 */
		double intersection_over_union(const DetectionObject &box_1, const DetectionObject &box_2);

		/**
		 * @brief Convert frame to blob.
		 * 
		 * @param frame The frame to be converted.
		 * @param infer_request The inference request.
		 * @param input_name The name of the input.
		 * @param auto_resize Value to change the size of the frame.
		 */
		void frame_to_blob(const cv::Mat &frame, 
							InferenceEngine::InferRequest::Ptr &infer_request, 
							const std::string &input_name, bool auto_resize = false);

		/**
		 * @brief Parse Mobilenet-SSD neural network model output.
		 * 
		 * @param[in] blob The frame with the detected objects.
		 * @param[in] height Height in pixels of the image.
		 * @param[in] width Width in pixels of the image.
		 * @param[in] threshold Value below which objects will be discarded.
		 * @param[out] objects The detected objects.
		 */
		void parse_ssd_output(const InferenceEngine::Blob::Ptr &blob, 
							const unsigned long height, 
							const unsigned long width, 
							const float threshold, 
							std::vector<DetectionObject> &objects);

		/**
		 * @brief Parse YOLOv3 neural network model output.
		 * 
		 * @param[in] cnn_network The
		 * @param[in] output_name The name of the output layer.
		 * @param[in] blob The frame with the detected objects.
		 * @param[in] resized_img_h Height in pixels of the image resized.
		 * @param[in] resized_img_w Width in pixels of the image resized.
		 * @param[in] original_img_h Height in pixels of the image.
		 * @param[in] original_img_w Width in pixels of the image.
		 * @param[in] threshold Value below which objects will be discarded.
		 * @param[out] objects The detected objects.
		 */
		void parse_yolov3_output(const InferenceEngine::CNNNetwork &cnn_network, 
								const std::string &output_name, 
								const InferenceEngine::Blob::Ptr &blob, 
								const unsigned long resized_img_h, 
								const unsigned long resized_img_w, 
								const unsigned long original_img_h, 
								const unsigned long original_img_w, 
								const float threshold, 
								std::vector<DetectionObject> &objects);
};
#endif // OBJECT_DETECTION_OPENVINO__OPENVINO_HPP_
