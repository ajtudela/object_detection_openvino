/*
 * OPENVINO CLASS
 *
 * Copyright (c) 2020-2021 Alberto José Tudela Roldán <ajtudela@gmail.com>
 * 
 * This file is part of object_detection_openvino project.
 * 
 * All rights reserved.
 *
 */

#ifndef OPENVINO_H
#define OPENVINO_H

// C++
#include <map>
#include <string>
#include <vector>

// OpenVINO
#include <inference_engine.hpp>
#include <samples/ocv_common.hpp>

#ifdef WITH_EXTENSIONS
    #include <ext_list.hpp>
#endif

// Object detection
#include <object_detection_openvino/detectionObject.h>
#include <object_detection_openvino/yoloParams.h>

/**
 * @brief Implementation of the Intel OpenVino as a more standard API.
 * 
 */
class Openvino{
	public:

		/**
		 * @brief Constructor.
		 * 
		 */
		Openvino();

		/**
		 * @brief Destructor.
		 * 
		 */
		~Openvino();

		/**
		 * @brief Set the target device to load the neural network model.
		 * 
		 * @param device The name of the device.
		 */
		void setTargetDevice(std::string device);

		/**
		 * @brief Read the Intermediate Representation (IR) generated by the Model Optimizer to be loaded into the device.
		 * 
		 * @param modelFileName The filename of the model in XML format.
		 * @param binFileName The filename of the model in BIN format.
		 * @param labelFileName The filename of the labels used in the model.
		 */
		void setNetworkModel(std::string modelFileName, std::string binFileName, std::string labelFileName);

		/**
		 * @brief Configure the inputs and outputs of the neural network.
		 * 
		 * @param networkType The name of the neural network.
		 */
		void configureNetwork(std::string networkType);

		/**
		 * @brief Load the neural network model to the selected device.
		 * 
		 * @param device The name of the device.
		 */
		void loadModelToDevice(std::string device);

		/**
		 * @brief Create an asynchronous inference request.
		 * 
		 */
		void createAsyncInferRequest();

		/**
		 * @brief Start the asynchronous inference request in the next frame.
		 * 
		 */
		void startNextAsyncInferRequest();

		/**
		 * @brief Swap the asynchronous inference request between the current frame and the next one.
		 * 
		 */
		void swapAsyncInferRequest();

		/**
		 * @brief Check if the device is ready.
		 * 
		 * @return True if the device is ready. False, otherwise.
		 */
		bool isDeviceReady();

		/**
		 * @brief Get the labels of the neural network model.
		 * 
		 * @return The names of the labels.
		 */
		std::vector<std::string> getLabels();

		/**
		 * @brief Get the detected objects in the frame.
		 * 
		 * @param height Height in pixels of the bounding box surrounding the object.
		 * @param width Width in pixels of the bounding box surrounding the object.
		 * @param threshold Value below which objects will be discarded.
		 * @param iouThreshold Value of the intersection over union threshold in a range between 0 and 1.
		 * Above this value, the bounding boxes will be overlapped.
		 * @return The detected objects.
		 */
		std::vector<DetectionObject> getDetectionObjects(size_t height, size_t width, float threshold, float iouThreshold);

		/**
		 * @brief Convert the frame to an asynchronous inference request.
		 * 
		 * @param frame The frame to be converted.
		 * @param autoResize Value to change the size of the frame.
		 */
		void frameToNextInfer(const cv::Mat &frame, bool autoResize = false);

	private:

		/// Name of the input layer.
		std::string inputName_;

		/// Type of the neural network.
		std::string networkType_;

		/// Name of the device to load the neural network into.
		std::string deviceTarget_;

		/// Implementation of the coordinates of regions for YOLOv3 model.
		std::map<std::string, YoloParams> yoloParams_;

		/// Interface of an executable network.
		InferenceEngine::ExecutableNetwork infNetwork_;

		/// Interface of asynchronous infer request.
		InferenceEngine::InferRequest::Ptr asyncInferRequestCurr_, asyncInferRequestNext_;

		/// A collection that contains string as key, and OutputInfo smart pointer as value.
		InferenceEngine::OutputsDataMap outputInfo_;

		/// A collection that contains string as key, and InputInfo smart pointer as value.
		InferenceEngine::InputsDataMap inputInfo_;

		/// Information about the Neural Network and the related binary information. 
		InferenceEngine::CNNNetwork cnnNetwork_;

		/// Inference Engine Core entity.
		InferenceEngine::Core core_;

		/// Class labels of the neural network.
		std::vector<std::string> labels_;

		/**
		 * @brief Index of the entry.
		 * 
		 * @param side Height of the image. 
		 * @param lcoords Coordinates of the object.
		 * @param lclasses Number of classes in the neural network.
		 * @param location The location.
		 * @param entry The entry.
		 * @return Index of the entry.
		 */
		static int entryIndex(int side, int lcoords, int lclasses, int location, int entry);

		/**
		 * @brief Intersection over union of the bounding boxes.
		 * 
		 * @param box_1 First bounding box.
		 * @param box_2 Second boudning box.
		 * @return Ratio between the area of the overlap and area of the union. 
		 */
		double intersectionOverUnion(const DetectionObject &box_1, const DetectionObject &box_2);

		/**
		 * @brief Convert frame to blob.
		 * 
		 * @param frame The frame to be converted.
		 * @param inferRequest The inference request.
		 * @param inputName The name of the input.
		 * @param autoResize Value to change the size of the frame.
		 */
		void frameToBlob(const cv::Mat &frame, InferenceEngine::InferRequest::Ptr &inferRequest, const std::string &inputName, bool autoResize = false);

		/**
		 * @brief Parse Mobilenet-SSD neural network model output.
		 * 
		 * @param[in] blob The frame with the detected objects.
		 * @param[in] height Height in pixels of the image.
		 * @param[in] width Width in pixels of the image.
		 * @param[in] threshold Value below which objects will be discarded.
		 * @param[out] objects The detected objects.
		 */
		void parseSSDOutput(const InferenceEngine::Blob::Ptr &blob, const unsigned long height, const unsigned long width, const float threshold, std::vector<DetectionObject> &objects);

		/**
		 * @brief Parse YOLOv3 neural network model output.
		 * 
		 * @param[in] cnnNetwork The
		 * @param[in] outputName The name of the output layer.
		 * @param[in] blob The frame with the detected objects.
		 * @param[in] resizedImgH Height in pixels of the image resized.
		 * @param[in] resizedImgW Width in pixels of the image resized.
		 * @param[in] originalImgH Height in pixels of the image.
		 * @param[in] originalImgW Width in pixels of the image.
		 * @param[in] threshold Value below which objects will be discarded.
		 * @param[out] objects The detected objects.
		 */
		void parseYOLOV3Output(const InferenceEngine::CNNNetwork &cnnNetwork, const std::string &outputName, const InferenceEngine::Blob::Ptr &blob, const unsigned long resizedImgH, const unsigned long resizedImgW, const unsigned long originalImgH, const unsigned long originalImgW, const float threshold, std::vector<DetectionObject> &objects);
};
#endif
